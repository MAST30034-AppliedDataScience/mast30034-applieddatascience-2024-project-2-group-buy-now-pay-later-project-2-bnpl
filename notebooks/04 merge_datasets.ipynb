{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join customer/merchant/transaction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spark session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Data Merge\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.driver.memory\", \"12g\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"16G\")\n",
    "    .config(\"spark.executor.memory\", \"16G\")\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\",\"8\")\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"64MB\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.network.timeout\", \"600s\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in datasets\n",
    "# Load in merchant data (parquet)\n",
    "merchant = spark.read.parquet(\"../data/curated/part_1/clean_merchant.parquet\")\n",
    "\n",
    "# Load in merchant fraud (csv)\n",
    "merchant_fp = pd.read_csv(\"../data/tables/part_1/merchant_fraud_probability.csv\")\n",
    "merchant_fp = spark.createDataFrame(merchant_fp)\n",
    "\n",
    "# Load in consumer list (csv)\n",
    "consumer_cid = pd.read_csv(\"../data/tables/part_1/tbl_consumer.csv\", delimiter=\"|\")\n",
    "consumer_cid = spark.createDataFrame(consumer_cid)\n",
    "\n",
    "# Load in consumer fraud (csv)\n",
    "consumer_fp = pd.read_csv(\"../data/tables/part_1/consumer_fraud_probability.csv\")\n",
    "consumer_fp = spark.createDataFrame(consumer_fp)\n",
    "\n",
    "consumer_ud = spark.read.parquet(\"../data/tables/part_1/consumer_user_details.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Join customer data**\n",
    "\n",
    "`tbl_consumer` to `consumer_user_detail`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining user id to customers\n",
    "consumer_cid = consumer_cid.withColumn(\"postcode\", consumer_cid.postcode.cast('string')) # cast postcode to string\n",
    "consumer = consumer_cid.join(consumer_ud, on = \"consumer_id\", how = 'left')\n",
    "consumer_list = consumer.selectExpr(\"user_id\", \"cast(postcode as string) postcode\",)\n",
    "consumer.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Join customers and transaction data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read transaction dataset\n",
    "transaction1 = spark.read.parquet(\"../data/tables/part_2\")\n",
    "transaction2 = spark.read.parquet(\"../data/tables/part_3\")\n",
    "transaction3 = spark.read.parquet(\"../data/tables/part_4\")\n",
    "\n",
    "transaction = transaction1.union(transaction2).union(transaction3)\n",
    "transaction.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join customers to transactions\n",
    "transaction_consumer = transaction.join(consumer_list, on='user_id', how='left')\n",
    "transaction_consumer.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_no_transaction = consumer_list.join(transaction, on='user_id', how='left_anti')\n",
    "print(f\"Number of consumers that have not made a transaction: {consumer_no_transaction.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining customer transaction to merchant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add consumer fraud to transactions\n",
    "final_df = transaction_consumer.join(consumer_fp, on =['user_id', 'order_datetime'], how = 'left').withColumnRenamed('fraud_probability', 'consumer_fraud')\n",
    "no_fraud = final_df.filter(F.col(\"consumer_fraud\").isNull()).count()\n",
    "print(f\"Number of transactions with no consumer fraud: {no_fraud:,}\")\n",
    "\n",
    "# Add merchant fraud to transactions by merchant and date\n",
    "final_df = final_df.join(merchant_fp, on=['merchant_abn','order_datetime'], how = 'left').withColumnRenamed('fraud_probability', 'merchant_fraud')\n",
    "no_fraud = final_df.filter(F.col(\"merchant_fraud\").isNull()).count()\n",
    "print(f\"Number of transactions with no merchant fraud: {no_fraud:,}\")\n",
    "\n",
    "# Impute all null fraud probabilities as 0\n",
    "final_df = final_df.fillna(0, subset=['merchant_fraud', 'consumer_fraud'])\n",
    "no_fraud = final_df.filter((final_df[\"consumer_fraud\"]==0) & (final_df[\"merchant_fraud\"]==0)).count()\n",
    "print(f\"Number of transactions with no merchant fraud or consumer fraud: {no_fraud:,}\")\n",
    "\n",
    "# final_df.show(5) # hidden to prevent crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition the DataFrame to increase the number of tasks (partitions)\n",
    "# final_df.repartition(200).write.mode('overwrite').option(\"maxRecordsPerFile\", 50000).parquet('../data/curated/fraud_watch/')\n",
    "#TODO: ERROR NEEDS FIXING :( CANNOT SAVE CUSTOMERS-TRANSACTIONS/FINAL_DF FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join external datasets\n",
    "Here, we estimate weekly disposible income based on the difference between total_personal_income and the average spent on rent or morgage repayments per week. The calculation uses weekly variables as follows.\n",
    "$$\\text{weekly disposible income} = \\text{total personal income} - (\\text{median rent} \\times \\text{proportion of renters}) - (\\text{median morgage repayment} \\times \\text{proportion of mortgage holders})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download housing data\n",
    "file_path = '../data/tables/sa2_dataset/main/C21_G37_SA2.csv'\n",
    "url = \"https://api.data.abs.gov.au/data/C21_G37_SA2/1+2+R_T+_T...SA2..2021.?detail=full\"\n",
    "headers = {'accept': 'text/csv'}\n",
    "\n",
    "# Download file\n",
    "response = requests.get(url, headers=headers, stream=True)\n",
    "\n",
    "with open(file_path, 'wb') as file:\n",
    "    for chunk in response.iter_content(chunk_size=8192):\n",
    "        if chunk:\n",
    "            file.write(chunk)\n",
    "\n",
    "# Read in data and cast types\n",
    "data = pd.read_csv(file_path, dtype={\"REGION\": object})\n",
    "\n",
    "variables = {\"R_T\":'renting',\n",
    "             \"2\":'owned_mortgage',\n",
    "             \"1\": 'owned_outright',\n",
    "             \"_T\":'total_responses',\n",
    "             \"REGION\": 'sa2_code'}\n",
    "\n",
    "# Aggregate to ignore the 'dwelling type' feature\n",
    "tenure_data = data.groupby(['REGION', 'TENLLD']).agg('sum').reset_index()[['REGION', 'TENLLD', 'OBS_VALUE']]\n",
    "tenure_data = tenure_data.pivot(index='REGION', columns='TENLLD', values='OBS_VALUE').reset_index()\n",
    "tenure_data = tenure_data.rename(variables, axis=1)\n",
    "tenure_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply calculations\n",
    "tenure_data['percent_mortgage'] = tenure_data['owned_mortgage'] / tenure_data['total_responses']\n",
    "tenure_data['percent_rent'] = tenure_data['renting'] / tenure_data['total_responses']\n",
    "\n",
    "# Investibate number of records with missing data\n",
    "zero_responses = tenure_data[tenure_data.isna().any(axis=1)]\n",
    "print('Number of regions with no reponses for housing section: ', len(zero_responses))\n",
    "zero_responses.head(5)\n",
    "\n",
    "# Handle missing null values by setting to zero\n",
    "percentage_tenure = tenure_data.fillna(0, axis=1).iloc[:,[0, -1, -2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataset with median statistics\n",
    "variables = {1: \"median_age\", \n",
    "             2: \"median_total_personal_income\",\n",
    "             3: \"median_total_family_income\",\n",
    "             4: \"median_total_household_income\",\n",
    "             5: \"median_mortgage_repayment\",\n",
    "             6: \"median_rent\",\n",
    "             7: \"avg_people_per_bedroom\",\n",
    "             8: \"avg_household_size\"}\n",
    "\n",
    "# Read in data\n",
    "medians = pd.read_csv(\"../data/curated/sa2_dataset/C21_G02_SA2_clean.csv\")\n",
    "\n",
    "# Restructure table\n",
    "medians = medians.pivot(index='sa2_code', columns=['type_of_value_code'], values='obs_value').reset_index().rename(columns=variables)\n",
    "medians.columns.name = None\n",
    "medians['sa2_code'] = medians.sa2_code.astype(str)\n",
    "medians.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before joining housing data to this table of medians, we investigate the records and notice that there are records that have null median summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in list of SA2 codes and associated names\n",
    "col_types = {\"POSTCODE\": str, \"SA2_CODE_2021\":str, \"RATIO_FROM_TO\": float}\n",
    "sa2_names = pd.read_excel(\"../data/tables/correspondence/CG_POSTCODE_2021_SA2_2021.xlsx\", converters=col_types)[['SA2_CODE_2021', 'SA2_NAME_2021', 'POSTCODE']]\n",
    "\n",
    "# Find records with null columns\n",
    "null_regions = medians[medians.isna().any(axis=1)]\n",
    "null_regions = null_regions.merge(sa2_names, left_on='sa2_code', right_on='SA2_CODE_2021')\n",
    "\n",
    "# Show the name of regions associated with null values \n",
    "null_regions.iloc[:,-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the external dataset is only missing values for SA2 regions marked as having \"no usual address\", and this is ok since they are special purpose codes that don't correspond with a postcode. We can avoid any joining issues by using inner joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEEKS_IN_MONTH = 4.345\n",
    "\n",
    "abs_df = medians.merge(percentage_tenure, on='sa2_code')\n",
    "\n",
    "# Calculate average weekly spending on housing per SA2 zone\n",
    "# note: monthly mortgage repayment converted to weekly by dividing by # weeks in a month\n",
    "abs_df['avg_housing_weekly'] = abs_df.median_rent*abs_df.percent_rent + abs_df.median_mortgage_repayment*(abs_df.percent_mortgage/WEEKS_IN_MONTH)\n",
    "abs_df['weekly_personal_disposable'] = abs_df.median_total_personal_income - abs_df.avg_housing_weekly\n",
    "\n",
    "# Associate SA2 zones to postcodes using \"correspondence.parquet\"\n",
    "abs_df = abs_df.merge(sa2_names, left_on='sa2_code', right_on='SA2_CODE_2021').drop('SA2_CODE_2021', axis=1)\n",
    "\n",
    "# Rename columns for consistency\n",
    "abs_df.rename({\"SA2_NAME_2021\":'sa2_name', \"POSTCODE\":'postcode'}, axis=1, inplace=True)\n",
    "abs_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "abs_df.to_parquet('../data/curated/sa2_dataset/abs_medians.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join ABS and customer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in abs demographic data and customer/transaction data\n",
    "abs_df = spark.read.parquet(\"../data/curated/sa2_dataset/abs_medians.parquet\")\n",
    "\n",
    "customer_details_abs = final_df.join(abs_df, on='postcode')\n",
    "customer_details_abs.repartition(200).write.mode('overwrite').parquet('../data/curated/all_details/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
