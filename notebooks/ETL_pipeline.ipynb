{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading library\n",
    "import os\n",
    "os.sys.path.append(\"../\")\n",
    "from scripts.etl_pipeline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 23:45:40 WARN Utils: Your hostname, qinsitaodeMacBook-Air.local resolves to a loopback address: 127.0.0.1; using 100.92.15.134 instead (on interface en0)\n",
      "24/08/29 23:45:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/29 23:45:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/08/29 23:45:42 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark Session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"ETL Pipeline\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.execturo.memory\", \"2g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can't use `urlretrieve` to get the data from Canvas, please download it to your local machine and move it `data/tables`. Then run the code below to unzip the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign data path\n",
    "raw_path = \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for file in os.listdir(f\"{raw_path}/tables\"):\n",
    "    if file == \".gitkeep\":\n",
    "        continue\n",
    "    with zipfile.ZipFile(f\"{raw_path}/tables/{file}\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(f\"{raw_path}/\")\n",
    "    os.remove(f\"{raw_path}/tables/{file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform\n",
    "\n",
    "The system use `user_id` as a key for identifying customer in transactions record and fraud probability tables. However, they also have a key-value map of `user_id` and `consumer_id`. We will use `consumer_id` as the only ID for customer. Thus, we will map `user_id` from each table to `consumer_id` and drop the former.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 23:45:55 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# Load consumer info - a key : value map for user_id to consumer_id\n",
    "consumer_info = spark.read.parquet(f\"{raw_path}/tables/consumer_user_details.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load all files that need to replace user_id\n",
    "consumer_fraud_rate = spark.read.csv(f\"{raw_path}/tables/consumer_fraud_probability.csv\", header=True, inferSchema=True)\n",
    "consumer_fraud_rate = replace_id(consumer_info, consumer_fraud_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load transaction data for user_id replacement\n",
    "transaction_p1 = spark.read.parquet(f\"{raw_path}/tables/transactions_20210228_20210827_snapshot\")\n",
    "transaction_p1 = replace_id(consumer_info, transaction_p1)\n",
    "\n",
    "transaction_p2 = spark.read.parquet(f\"{raw_path}/tables/transactions_20210828_20220227_snapshot\")\n",
    "transaction_p2 = replace_id(consumer_info, transaction_p2)\n",
    "\n",
    "transaction_p3 = spark.read.parquet(f\"{raw_path}/tables/transactions_20220228_20220828_snapshot\")\n",
    "transaction_p3 = replace_id(consumer_info, transaction_p3)\n",
    "\n",
    "transaction_records = reduce(DataFrame.unionAll, [transaction_p1, transaction_p2, transaction_p3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that replacing `user_id` to `consumer_id` is done, load all other data and clean them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load merchant fraud probability\n",
    "# merchant_fraud_rate = spark.read.csv(f\"{raw_path}/tables/merchant_fraud_probability.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# date_pattern = r\"^\\d{4}-\\d{2}-\\d{2}$\"\n",
    "\n",
    "# test = merchant_fraud_rate.withColumn(\"is_valid_date\", F.regexp_extract(F.col(\"order_datetime\"), date_pattern, 0))\n",
    "# invalid_dates = test.filter(F.col(\"is_valid_date\") == \"\")\n",
    "# invalid_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of merchants: 4422\n",
      "Total number of merchants with fraudulent probability: 61\n"
     ]
    }
   ],
   "source": [
    "merchant_fraud = spark.read.csv(f\"{raw_path}/tables/merchant_fraud_probability.csv\", header=True, inferSchema=True)\n",
    "print(f'Total number of merchants: {transaction_records.select(\"merchant_abn\").distinct().count()}')\n",
    "print(f'Total number of merchants with fraudulent probability: {merchant_fraud.select(\"merchant_abn\").distinct().count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning `tbl_merchants.parquet`. The feature `tags` is a string that represent either a tuple or a list, containing 3 elements:\n",
    "* Items that are being sold\n",
    "* Revenue levels\n",
    "* Commission rate\n",
    "Each elements either a list, a tuple, or a combination of both (e.g starts with `[` and ends with `)` and vice versa). These inconsistencies are mostly due to human errors. Thus, we need to take into account these consistent when splitting the values of the feature `tags` into separate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- merchant_abn: long (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- revenue_level: string (nullable = true)\n",
      " |-- take_rate: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>name</th><th>merchant_abn</th><th>category</th><th>revenue_level</th><th>take_rate</th></tr>\n",
       "<tr><td>Felis Limited</td><td>10023283211</td><td>furniture, home f...</td><td>e</td><td>0.18</td></tr>\n",
       "<tr><td>Arcu Ac Orci Corp...</td><td>10142254217</td><td>cable, satellite,...</td><td>b</td><td>4.22</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+------------+--------------------+-------------+---------+\n",
       "|                name|merchant_abn|            category|revenue_level|take_rate|\n",
       "+--------------------+------------+--------------------+-------------+---------+\n",
       "|       Felis Limited| 10023283211|furniture, home f...|            e|     0.18|\n",
       "|Arcu Ac Orci Corp...| 10142254217|cable, satellite,...|            b|     4.22|\n",
       "+--------------------+------------+--------------------+-------------+---------+"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load merchant's info and clean it\n",
    "merchant_info = spark.read.parquet(f\"{raw_path}/tables/tbl_merchants.parquet\")\n",
    "merchant_info = load_merchant_details(merchant_info)\n",
    "merchant_info.printSchema()\n",
    "merchant_info.limit(2)\n",
    "# merchant_info.groupBy(F.col(\"revenue_level\")).agg(F.avg(F.col(\"take_rate\"))) # average commission rate of each revenue level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>name</th><th>consumer_id</th><th>gender</th><th>state</th><th>postcode</th></tr>\n",
       "<tr><td>Yolanda Williams</td><td>1195503</td><td>Female</td><td>WA</td><td>6935</td></tr>\n",
       "<tr><td>Mary Smith</td><td>179208</td><td>Female</td><td>NSW</td><td>2782</td></tr>\n",
       "<tr><td>Jill Jones MD</td><td>1194530</td><td>Female</td><td>NT</td><td>862</td></tr>\n",
       "<tr><td>Lindsay Jimenez</td><td>154128</td><td>Female</td><td>NSW</td><td>2780</td></tr>\n",
       "<tr><td>Rebecca Blanchard</td><td>712975</td><td>Female</td><td>WA</td><td>6355</td></tr>\n",
       "<tr><td>Karen Chapman</td><td>407340</td><td>Female</td><td>NSW</td><td>2033</td></tr>\n",
       "<tr><td>Andrea Jones</td><td>511685</td><td>Female</td><td>QLD</td><td>4606</td></tr>\n",
       "<tr><td>Stephen Williams</td><td>448088</td><td>Male</td><td>WA</td><td>6056</td></tr>\n",
       "<tr><td>Stephanie Reyes</td><td>650435</td><td>Female</td><td>NSW</td><td>2482</td></tr>\n",
       "<tr><td>Jillian Gonzales</td><td>1058499</td><td>Female</td><td>VIC</td><td>3220</td></tr>\n",
       "<tr><td>Eugene Lucas</td><td>428325</td><td>Undisclosed</td><td>VIC</td><td>3063</td></tr>\n",
       "<tr><td>Melissa Jones</td><td>1494640</td><td>Female</td><td>WA</td><td>6743</td></tr>\n",
       "<tr><td>Angela Brown PhD</td><td>1146717</td><td>Female</td><td>QLD</td><td>4673</td></tr>\n",
       "<tr><td>Lance Butler</td><td>1343547</td><td>Male</td><td>VIC</td><td>3332</td></tr>\n",
       "<tr><td>Paul Abbott</td><td>1463076</td><td>Male</td><td>QLD</td><td>4512</td></tr>\n",
       "<tr><td>Tracy Hart</td><td>1356405</td><td>Male</td><td>NSW</td><td>2452</td></tr>\n",
       "<tr><td>Alyssa Wilson</td><td>1331093</td><td>Female</td><td>VIC</td><td>3719</td></tr>\n",
       "<tr><td>Michael Burnett</td><td>80965</td><td>Male</td><td>NSW</td><td>1109</td></tr>\n",
       "<tr><td>Victoria Gonzalez</td><td>1226530</td><td>Female</td><td>TAS</td><td>7276</td></tr>\n",
       "<tr><td>James Norris</td><td>1390367</td><td>Undisclosed</td><td>VIC</td><td>3234</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+-----------------+-----------+-----------+-----+--------+\n",
       "|             name|consumer_id|     gender|state|postcode|\n",
       "+-----------------+-----------+-----------+-----+--------+\n",
       "| Yolanda Williams|    1195503|     Female|   WA|    6935|\n",
       "|       Mary Smith|     179208|     Female|  NSW|    2782|\n",
       "|    Jill Jones MD|    1194530|     Female|   NT|     862|\n",
       "|  Lindsay Jimenez|     154128|     Female|  NSW|    2780|\n",
       "|Rebecca Blanchard|     712975|     Female|   WA|    6355|\n",
       "|    Karen Chapman|     407340|     Female|  NSW|    2033|\n",
       "|     Andrea Jones|     511685|     Female|  QLD|    4606|\n",
       "| Stephen Williams|     448088|       Male|   WA|    6056|\n",
       "|  Stephanie Reyes|     650435|     Female|  NSW|    2482|\n",
       "| Jillian Gonzales|    1058499|     Female|  VIC|    3220|\n",
       "|     Eugene Lucas|     428325|Undisclosed|  VIC|    3063|\n",
       "|    Melissa Jones|    1494640|     Female|   WA|    6743|\n",
       "| Angela Brown PhD|    1146717|     Female|  QLD|    4673|\n",
       "|     Lance Butler|    1343547|       Male|  VIC|    3332|\n",
       "|      Paul Abbott|    1463076|       Male|  QLD|    4512|\n",
       "|       Tracy Hart|    1356405|       Male|  NSW|    2452|\n",
       "|    Alyssa Wilson|    1331093|     Female|  VIC|    3719|\n",
       "|  Michael Burnett|      80965|       Male|  NSW|    1109|\n",
       "|Victoria Gonzalez|    1226530|     Female|  TAS|    7276|\n",
       "|     James Norris|    1390367|Undisclosed|  VIC|    3234|\n",
       "+-----------------+-----------+-----------+-----+--------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load consumer's info and reformat\n",
    "consumer_info = spark.read.csv(f\"{raw_path}/tables/tbl_consumer.csv\", header=True, inferSchema=True)\n",
    "consumer_info = load_consumer_details(consumer_info)\n",
    "\n",
    "# consumer_info.groupBy(\"gender\").count() # relatively same proportion of female and male customer, only a small percentage of did not provide their gender\n",
    "consumer_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_ALREADY_EXISTS] Path file:/Users/qinsitao/Documents/GitHub/project-2-group-buy-now-pay-later-industry-project-9/data/curated/consumer_info.parquet already exists. Set mode as \"overwrite\" to overwrite the existing path.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Prepare and load data for geosparial analysis\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Skip if the file exists\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/curated/consumer_info.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mconsumer_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/curated/consumer_info.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/curated/consumer_fraud_rate.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      8\u001b[0m     consumer_fraud_rate\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/curated/consumer_fraud_rate.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1721\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.11/site-packages/pyspark/sql/readwriter.py?line=1718'>1719</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.11/site-packages/pyspark/sql/readwriter.py?line=1719'>1720</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_opts(compression\u001b[39m=\u001b[39mcompression)\n\u001b[0;32m-> <a href='file:///usr/local/lib/python3.11/site-packages/pyspark/sql/readwriter.py?line=1720'>1721</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49mparquet(path)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.11/site-packages/py4j/java_gateway.py?line=1315'>1316</a>\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.11/site-packages/py4j/java_gateway.py?line=1316'>1317</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.11/site-packages/py4j/java_gateway.py?line=1317'>1318</a>\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.11/site-packages/py4j/java_gateway.py?line=1318'>1319</a>\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.11/site-packages/py4j/java_gateway.py?line=1320'>1321</a>\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> <a href='file:///usr/local/lib/python3.11/site-packages/py4j/java_gateway.py?line=1321'>1322</a>\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.11/site-packages/py4j/java_gateway.py?line=1322'>1323</a>\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.11/site-packages/py4j/java_gateway.py?line=1324'>1325</a>\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.11/site-packages/py4j/java_gateway.py?line=1325'>1326</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py?line=180'>181</a>\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py?line=181'>182</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py?line=182'>183</a>\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py?line=183'>184</a>\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py?line=184'>185</a>\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py?line=185'>186</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py?line=186'>187</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path file:/Users/qinsitao/Documents/GitHub/project-2-group-buy-now-pay-later-industry-project-9/data/curated/consumer_info.parquet already exists. Set mode as \"overwrite\" to overwrite the existing path."
     ]
    }
   ],
   "source": [
    "# Prepare and load data for geosparial analysis\n",
    "# Skip if the file exists\n",
    "if not os.path.isfile(\"../data/curated/consumer_info.parquet\"):\n",
    "    consumer_info.write.parquet('../data/curated/consumer_info.parquet')\n",
    "\n",
    "\n",
    "if not os.path.isfile(\"../data/curated/consumer_fraud_rate.parquet\"):\n",
    "    consumer_fraud_rate.write.parquet('../data/curated/consumer_fraud_rate.parquet')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
