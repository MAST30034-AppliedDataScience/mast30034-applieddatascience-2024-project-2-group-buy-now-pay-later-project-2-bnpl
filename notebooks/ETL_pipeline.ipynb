{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading library\n",
    "import os\n",
    "os.sys.path.append(\"../\")\n",
    "from scripts.etl_pipeline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark Session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"ETL Pipeline\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can't use `urlretrieve` to get the data from Canvas, please download it to your local machine and move it `data/tables`. Then run the code below to unzip the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor file in os.listdir(f\"{raw_path}/tables\"):\\n    if file == \".gitkeep\":\\n        continue\\n    with zipfile.ZipFile(f\"{raw_path}/tables/{file}\", \"r\") as zip_ref:\\n        zip_ref.extractall(f\"{raw_path}/\")\\n    os.remove(f\"{raw_path}/tables/{file}\")\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign data path\n",
    "raw_path = \"../data\"\n",
    "\n",
    "# Unzip files (Only run once)\n",
    "\"\"\"\n",
    "for file in os.listdir(f\"{raw_path}/tables\"):\n",
    "    if file == \".gitkeep\":\n",
    "        continue\n",
    "    with zipfile.ZipFile(f\"{raw_path}/tables/{file}\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(f\"{raw_path}/\")\n",
    "    os.remove(f\"{raw_path}/tables/{file}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform\n",
    "\n",
    "The system use `user_id` as a key for identifying customer in transactions record and fraud probability tables. However, they also have a key-value map of `user_id` and `consumer_id`. We will use `consumer_id` as the only ID for customer. Thus, we will map `user_id` from each table to `consumer_id` and drop the former.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>user_id</th><th>consumer_id</th></tr>\n",
       "<tr><td>1</td><td>1195503</td></tr>\n",
       "<tr><td>2</td><td>179208</td></tr>\n",
       "<tr><td>3</td><td>1194530</td></tr>\n",
       "<tr><td>4</td><td>154128</td></tr>\n",
       "<tr><td>5</td><td>712975</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+-----------+\n",
       "|user_id|consumer_id|\n",
       "+-------+-----------+\n",
       "|      1|    1195503|\n",
       "|      2|     179208|\n",
       "|      3|    1194530|\n",
       "|      4|     154128|\n",
       "|      5|     712975|\n",
       "+-------+-----------+"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load consumer user details -> a key:value map for user_id to consumer_id\n",
    "consumer_user_map = spark.read.parquet(f\"{raw_path}/tables/consumer_user_details.parquet\")\n",
    "consumer_user_map.limit(5) # Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-----------------+\n",
      "|user_id|order_datetime|fraud_probability|\n",
      "+-------+--------------+-----------------+\n",
      "|   6228|    2021-12-19| 97.6298077657765|\n",
      "|  21419|    2021-12-10|99.24738020302328|\n",
      "|   5606|    2021-10-17|84.05825045251777|\n",
      "|   3101|    2021-04-17|91.42192091901347|\n",
      "|  22239|    2021-10-19|94.70342477508035|\n",
      "+-------+--------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "The dataset count is  34864\n",
      "+--------------+------------------+-----------+\n",
      "|order_datetime| fraud_probability|consumer_id|\n",
      "+--------------+------------------+-----------+\n",
      "|    2022-02-20| 9.805431136520959|    1195503|\n",
      "|    2021-08-30| 9.599513915425788|     179208|\n",
      "|    2021-09-25|10.069850934775245|     179208|\n",
      "|    2021-11-03| 8.300636455314633|    1194530|\n",
      "|    2021-10-09| 9.633302411090419|     154128|\n",
      "+--------------+------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "The dataset count is  34864\n"
     ]
    }
   ],
   "source": [
    "# Load consumer fraud rate dataset\n",
    "consumer_fraud_rate = spark.read.csv(f\"{raw_path}/tables/consumer_fraud_probability.csv\", header=True, inferSchema=True)\n",
    "consumer_fraud_rate.show(5)\n",
    "get_dataset_count(consumer_fraud_rate)\n",
    "\n",
    "# Replace all user_id with unique consumer_id\n",
    "consumer_fraud_rate = replace_id(consumer_user_map, consumer_fraud_rate)\n",
    "consumer_fraud_rate.show(5)\n",
    "\n",
    "# Check to make sure no rows were lost on the inner join\n",
    "get_dataset_count(consumer_fraud_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is no change in the number of entries upon an inner join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset count is  14195505\n",
      "The dataset count is  14195505\n"
     ]
    }
   ],
   "source": [
    "# Load all the transaction data \n",
    "transaction_p1 = spark.read.parquet(f\"{raw_path}/tables/transactions_20210228_20210827_snapshot\")\n",
    "transaction_p2 = spark.read.parquet(f\"{raw_path}/tables/transactions_20210828_20220227_snapshot\")\n",
    "transaction_p3 = spark.read.parquet(f\"{raw_path}/tables/transactions_20220228_20220828_snapshot\")\n",
    "\n",
    "# Combine the datasets\n",
    "transaction_records = reduce(DataFrame.unionAll, [transaction_p1, transaction_p2, transaction_p3])\n",
    "get_dataset_count(transaction_records)\n",
    "\n",
    "# Replace user_id with consumer_id after combining\n",
    "transaction_records = replace_id(consumer_user_map, transaction_records)\n",
    "\n",
    "# Check to make sure no rows were lost on the inner join\n",
    "get_dataset_count(transaction_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that replacing `user_id` to `consumer_id` is done, load all other data and clean them. We start off with the merchant fraud probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+------------------+\n",
      "|merchant_abn|order_datetime| fraud_probability|\n",
      "+------------+--------------+------------------+\n",
      "| 19492220327|    2021-11-28|44.403658647495355|\n",
      "| 31334588839|    2021-10-02| 42.75530083865367|\n",
      "| 19492220327|    2021-12-22|38.867790051131095|\n",
      "| 82999039227|    2021-12-19|  94.1347004808891|\n",
      "| 90918180829|    2021-09-02| 43.32551731714902|\n",
      "+------------+--------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "The dataset count is  114\n"
     ]
    }
   ],
   "source": [
    "# Load consumer fraud rate dataset\n",
    "merchant_fraud_rate = spark.read.csv(f\"{raw_path}/tables/merchant_fraud_probability.csv\", header=True, inferSchema=True)\n",
    "merchant_fraud_rate.show(5)\n",
    "get_dataset_count(merchant_fraud_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning `tbl_merchants.parquet`. The feature `tags` is a string that represents either a tuple or a list, containing 3 elements:\n",
    "* Items that are being sold\n",
    "* Revenue levels\n",
    "* Commission rate\n",
    "\n",
    "Each elements either a list, a tuple, or a combination of both (e.g starts with `[` and ends with `)` and vice versa). These inconsistencies are mostly due to human errors. Thus, we need to take into account these consistent when splitting the values of the feature `tags` into separate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+-----------------------------------------------------------------------------------------------------------------+------------+\n",
      "|name                                |tags                                                                                                             |merchant_abn|\n",
      "+------------------------------------+-----------------------------------------------------------------------------------------------------------------+------------+\n",
      "|Felis Limited                       |((furniture, home furnishings and equipment shops, and manufacturers, except appliances), (e), (take rate: 0.18))|10023283211 |\n",
      "|Arcu Ac Orci Corporation            |([cable, satellite, and otHer pay television and radio services], [b], [take rate: 4.22])                        |10142254217 |\n",
      "|Nunc Sed Company                    |([jewelry, watch, clock, and silverware shops], [b], [take rate: 4.40])                                          |10165489824 |\n",
      "|Ultricies Dignissim Lacus Foundation|([wAtch, clock, and jewelry repair shops], [b], [take rate: 3.29])                                               |10187291046 |\n",
      "|Enim Condimentum PC                 |([music shops - musical instruments, pianos, and sheet music], [a], [take rate: 6.33])                           |10192359162 |\n",
      "+------------------------------------+-----------------------------------------------------------------------------------------------------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Before: \n",
      "The dataset count is  4026\n",
      "After: \n",
      "The dataset count is  4026\n",
      "+------------------------------------+------------+-------------------------------------------------------------------------------------+-------------+---------+\n",
      "|name                                |merchant_abn|category                                                                             |revenue_level|take_rate|\n",
      "+------------------------------------+------------+-------------------------------------------------------------------------------------+-------------+---------+\n",
      "|Felis Limited                       |10023283211 |furniture, home furnishings and equipment shops, and manufacturers, except appliances|e            |0.18     |\n",
      "|Arcu Ac Orci Corporation            |10142254217 |cable, satellite, and other pay television and radio services                        |b            |4.22     |\n",
      "|Nunc Sed Company                    |10165489824 |jewelry, watch, clock, and silverware shops                                          |b            |4.4      |\n",
      "|Ultricies Dignissim Lacus Foundation|10187291046 |watch, clock, and jewelry repair shops                                               |b            |3.29     |\n",
      "|Enim Condimentum PC                 |10192359162 |music shops - musical instruments, pianos, and sheet music                           |a            |6.33     |\n",
      "+------------------------------------+------------+-------------------------------------------------------------------------------------+-------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load merchant's info\n",
    "merchant_info = spark.read.parquet(f\"{raw_path}/tables/tbl_merchants.parquet\")\n",
    "merchant_info.show(5, truncate=False)\n",
    "\n",
    "# Clean the data\n",
    "merchant_info = clean_merchant_details(merchant_info)\n",
    "merchant_info.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data on consumer's basic information is a single column that contains the consumer's name, address, state, postcode, gender, and their unqiue consumer ID, each separated by \"`|`\". Thus, we will need to split these into individual columns. Based on the `README.md` for the data, we will only keep the consumer's name, state, postcode, gender, and consumer ID as the addresses are fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------+\n",
      "|name|address|state|postcode|gender|consumer_id                       |\n",
      "+---------------------------------------------------------------------+\n",
      "|Yolanda Williams|413 Haney Gardens Apt. 742|WA|6935|Female|1195503   |\n",
      "|Mary Smith|3764 Amber Oval|NSW|2782|Female|179208                    |\n",
      "|Jill Jones MD|40693 Henry Greens|NT|862|Female|1194530               |\n",
      "|Lindsay Jimenez|00653 Davenport Crossroad|NSW|2780|Female|154128     |\n",
      "|Rebecca Blanchard|9271 Michael Manors Suite 651|WA|6355|Female|712975|\n",
      "+---------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Before: \n",
      "The dataset count is  499999\n",
      "After: \n",
      "The dataset count is  499999\n",
      "+-----------------+-----------+------+-----+--------+\n",
      "|name             |consumer_id|gender|state|postcode|\n",
      "+-----------------+-----------+------+-----+--------+\n",
      "|Yolanda Williams |1195503    |Female|WA   |6935    |\n",
      "|Mary Smith       |179208     |Female|NSW  |2782    |\n",
      "|Jill Jones MD    |1194530    |Female|NT   |862     |\n",
      "|Lindsay Jimenez  |154128     |Female|NSW  |2780    |\n",
      "|Rebecca Blanchard|712975     |Female|WA   |6355    |\n",
      "+-----------------+-----------+------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load consumer info's\n",
    "consumer_info = spark.read.csv(f\"{raw_path}/tables/tbl_consumer.csv\", header=True, inferSchema=True)\n",
    "consumer_info.show(5, truncate=False)\n",
    "\n",
    "# Clean the data\n",
    "consumer_info = clean_consumer_details(consumer_info)\n",
    "consumer_info.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need to ensure that the datetime of all dataset with such column is within the specified range (labeled on the name of the intial downloaded file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting entries: 14195505 \n",
      "Final entries: 12561377\n",
      "Net change (%): 11.51 \n"
     ]
    }
   ],
   "source": [
    "transaction_records = ensure_datetime_range(transaction_records, \"2021-02-28\", \"2022-08-28\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting entries: 114 \n",
      "Final entries: 114\n",
      "Net change (%): 0.0 \n"
     ]
    }
   ],
   "source": [
    "merchant_fraud_rate = ensure_datetime_range(merchant_fraud_rate, \"2021-02-08\", \"2022-08-28\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting entries: 34864 \n",
      "Final entries: 34864\n",
      "Net change (%): 0.0 \n"
     ]
    }
   ],
   "source": [
    "consumer_fraud_rate = ensure_datetime_range(consumer_fraud_rate, \"2021-02-08\", \"2022-08-28\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we check for any existing null values across all dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------------------------+-------------------------+\n",
      "|order_datetime_missing_count|fraud_probability_missing_count|consumer_id_missing_count|\n",
      "+----------------------------+-------------------------------+-------------------------+\n",
      "|                           0|                              0|                        0|\n",
      "+----------------------------+-------------------------------+-------------------------+\n",
      "\n",
      "+--------------------------+----------------------------+-------------------------------+\n",
      "|merchant_abn_missing_count|order_datetime_missing_count|fraud_probability_missing_count|\n",
      "+--------------------------+----------------------------+-------------------------------+\n",
      "|                         0|                           0|                              0|\n",
      "+--------------------------+----------------------------+-------------------------------+\n",
      "\n",
      "+------------------+-------------------------+--------------------+-------------------+----------------------+\n",
      "|name_missing_count|consumer_id_missing_count|gender_missing_count|state_missing_count|postcode_missing_count|\n",
      "+------------------+-------------------------+--------------------+-------------------+----------------------+\n",
      "|                 0|                        0|                   0|                  0|                     0|\n",
      "+------------------+-------------------------+--------------------+-------------------+----------------------+\n",
      "\n",
      "+------------------+--------------------------+----------------------+---------------------------+-----------------------+\n",
      "|name_missing_count|merchant_abn_missing_count|category_missing_count|revenue_level_missing_count|take_rate_missing_count|\n",
      "+------------------+--------------------------+----------------------+---------------------------+-----------------------+\n",
      "|                 0|                         0|                     0|                          0|                      0|\n",
      "+------------------+--------------------------+----------------------+---------------------------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 208:=============================================>        (50 + 10) / 60]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+--------------------------+----------------------+----------------------------+-------------------------+\n",
      "|merchant_abn_missing_count|dollar_value_missing_count|order_id_missing_count|order_datetime_missing_count|consumer_id_missing_count|\n",
      "+--------------------------+--------------------------+----------------------+----------------------------+-------------------------+\n",
      "|                         0|                         0|                     0|                           0|                        0|\n",
      "+--------------------------+--------------------------+----------------------+----------------------------+-------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "calculate_missing_values(consumer_fraud_rate)\n",
    "calculate_missing_values(merchant_fraud_rate)\n",
    "calculate_missing_values(consumer_info)\n",
    "calculate_missing_values(merchant_info)\n",
    "calculate_missing_values(transaction_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, there are no mising values after we do some cleaning. We will come back to this after we merged the data together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "landing_directory = \"../data/curated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "consumer_fraud_rate.write.parquet(f\"{landing_directory}/consumer_fraud_prob.parquet\")\n",
    "merchant_fraud_rate.write.parquet(f\"{landing_directory}/merchant_fraud_prob.parquet\")\n",
    "transaction_records.write.parquet(f\"{landing_directory}/transactions.parquet\")\n",
    "merchant_info.write.parquet(f\"{landing_directory}/merchant_info.parquet\")\n",
    "consumer_info.write.parquet(f\"{landing_directory}/consumer_info.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
