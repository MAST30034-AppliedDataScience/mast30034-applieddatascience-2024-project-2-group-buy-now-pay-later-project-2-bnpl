{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading library\n",
    "from functools import reduce\n",
    "from pyspark.sql import SparkSession, DataFrame, functions as F\n",
    "from pyspark.sql.types import DoubleType, StringType, IntegerType\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Create a Spark Session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"ETL Pipeline\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.execturo.memory\", \"2g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can't use `urlretrive` to get the data from Canvas, please download it to your local machine and move it `data/tables`. Then run the code below to unzip the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign data path\n",
    "raw_path = \"../data\"\n",
    "\n",
    "for file in os.listdir(f\"{raw_path}/tables\"):\n",
    "    if file == \".gitkeep\":\n",
    "        continue\n",
    "    with zipfile.ZipFile(f\"{raw_path}/tables/{file}\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(f\"{raw_path}/\")\n",
    "    os.remove(f\"{raw_path}/tables/{file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform\n",
    "\n",
    "The system use `user_id` as a key for identifying customer in transactions record and fraud probability tables. However, they also have a key-value map of `user_id` and `consumer_id`. We will use `consumer_id` as the only ID for customer. Thus, we will map `user_id` from each table to `consumer_id` and drop the former."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_id(map_df, target_df):\n",
    "    mapped_df = target_df.join(map_df, on=\"user_id\", how=\"inner\")\n",
    "    mapped_df = mapped_df.drop('user_id')\n",
    "    \n",
    "    return mapped_df\n",
    "\n",
    "def merchant_info_clean(df):\n",
    "    df = df.withColumn(\"tags\", F.regexp_replace(\"tags\", r\"^[\\(\\[]|[\\)\\]]$\", \"\")) # Remove the outermost bracket\n",
    "    df = df.withColumn(\"tags\", F.regexp_replace(\"tags\", r\"[\\)\\]],\\s*[\\(\\[]\", r\")\\|(\")) # Replacing the comma that seperate each touple/list into \"|\"\n",
    "\n",
    "# Split accorddingly \n",
    "    df = df.withColumn(\"tags\", F.split(\"tags\", \"\\|\")) \n",
    "    df = df.withColumns({\"category\": F.regexp_replace(F.col(\"tags\").getItem(0), r\"^[\\(\\[]|[\\)\\]]$\", \"\"),\n",
    "                                           \"revenue_level\": F.regexp_replace(F.col(\"tags\").getItem(1), r\"^[\\(\\[]|[\\)\\]]$\", \"\"),\n",
    "                                           \"take_rate\": F.regexp_extract(F.col(\"tags\").getItem(2), r\"take rate: (\\d+\\.\\d+)\",1).cast(DoubleType())\n",
    "                                          })\n",
    "    df = df.drop(\"tags\")\n",
    "\n",
    "    df = df.filter((F.col(\"revenue_level\") == \"a\") | (F.col(\"revenue_level\") == \"b\") | (F.col(\"revenue_level\") == \"c\") |\n",
    "                   (F.col(\"revenue_level\") == \"d\") | (F.col(\"revenue_level\") == \"e\"))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load consumer info - a key : value map for user_id to consumer_id\n",
    "consumer_info = spark.read.parquet(f\"{raw_path}/tables/consumer_user_details.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all files that need to replace user_id\n",
    "consumer_fraud_rate = spark.read.csv(f\"{raw_path}/tables/consumer_fraud_probability.csv\", header=True, inferSchema=True)\n",
    "consumer_fraud_rate = replace_id(consumer_info, consumer_fraud_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>order_datetime</th><th>fraud_probability</th><th>consumer_id</th></tr>\n",
       "<tr><td>2022-02-20</td><td>9.805431136520959</td><td>1195503</td></tr>\n",
       "<tr><td>2021-08-30</td><td>9.599513915425788</td><td>179208</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------+-----------------+-----------+\n",
       "|order_datetime|fraud_probability|consumer_id|\n",
       "+--------------+-----------------+-----------+\n",
       "|    2022-02-20|9.805431136520959|    1195503|\n",
       "|    2021-08-30|9.599513915425788|     179208|\n",
       "+--------------+-----------------+-----------+"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consumer_fraud_rate.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transaction data for user_id replacement\n",
    "transaction_p1 = spark.read.parquet(f\"{raw_path}/tables/transactions_20210228_20210827_snapshot\")\n",
    "transaction_p1 = replace_id(consumer_info, transaction_p1)\n",
    "\n",
    "transaction_p2 = spark.read.parquet(f\"{raw_path}/tables/transactions_20210828_20220227_snapshot\")\n",
    "transaction_p2 = replace_id(consumer_info, transaction_p2)\n",
    "\n",
    "transaction_p3 = spark.read.parquet(f\"{raw_path}/tables/transactions_20220228_20220828_snapshot\")\n",
    "transaction_p3 = replace_id(consumer_info, transaction_p3)\n",
    "\n",
    "transaction_records = reduce(DataFrame.unionAll, [transaction_p1, transaction_p2, transaction_p3])\n",
    "transaction_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>merchant_abn</th><th>dollar_value</th><th>order_id</th><th>order_datetime</th><th>consumer_id</th></tr>\n",
       "<tr><td>62191208634</td><td>63.255848959735246</td><td>949a63c8-29f7-4ab...</td><td>2021-08-20</td><td>651338</td></tr>\n",
       "<tr><td>15549624934</td><td>130.3505283105634</td><td>6a84c3cf-612a-457...</td><td>2021-08-20</td><td>179208</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------+------------------+--------------------+--------------+-----------+\n",
       "|merchant_abn|      dollar_value|            order_id|order_datetime|consumer_id|\n",
       "+------------+------------------+--------------------+--------------+-----------+\n",
       "| 62191208634|63.255848959735246|949a63c8-29f7-4ab...|    2021-08-20|     651338|\n",
       "| 15549624934| 130.3505283105634|6a84c3cf-612a-457...|    2021-08-20|     179208|\n",
       "+------------+------------------+--------------------+--------------+-----------+"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transaction_records.limit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that replacing `user_id` to `consumer_id` is done, load all other data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>month</th></tr>\n",
       "<tr><td>12</td></tr>\n",
       "<tr><td>1</td></tr>\n",
       "<tr><td>3</td></tr>\n",
       "<tr><td>9</td></tr>\n",
       "<tr><td>4</td></tr>\n",
       "<tr><td>8</td></tr>\n",
       "<tr><td>10</td></tr>\n",
       "<tr><td>11</td></tr>\n",
       "<tr><td>2</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+\n",
       "|month|\n",
       "+-----+\n",
       "|   12|\n",
       "|    1|\n",
       "|    3|\n",
       "|    9|\n",
       "|    4|\n",
       "|    8|\n",
       "|   10|\n",
       "|   11|\n",
       "|    2|\n",
       "+-----+"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load merchant fraud probability\n",
    "merchant_fraud_rate = spark.read.csv(f\"{raw_path}/tables/merchant_fraud_probability.csv\", header=True, inferSchema=True)\n",
    "test = merchant_fraud_rate.withColumns({\"year\": F.year(F.col(\"order_datetime\")),\n",
    "                                 \"month\": F.month(F.col(\"order_datetime\")),\n",
    "                                 \"day\": F.day(F.col(\"order_datetime\"))})\n",
    "test.select(\"month\").distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning `tbl_merchants.parquet`. The feature `tags` is a string that represent either a tuple or a list, containing 3 elements:\n",
    "* Items that are being sold\n",
    "* Revenue levels\n",
    "* Commission rate\n",
    "Each elements either a list, a tuple, or a combination of both (e.g starts with `[` and ends with `)` and vice versa). These inconsistencies are mostly due to human errors. Thus, we need to take into account these consistent when splitting the values of the feature `tags` into separate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>name</th><th>merchant_abn</th><th>category</th><th>revenue_level</th><th>take_rate</th></tr>\n",
       "<tr><td>Felis Limited</td><td>10023283211</td><td>furniture, home f...</td><td>e</td><td>0.18</td></tr>\n",
       "<tr><td>Arcu Ac Orci Corp...</td><td>10142254217</td><td>cable, satellite,...</td><td>b</td><td>4.22</td></tr>\n",
       "<tr><td>Nunc Sed Company</td><td>10165489824</td><td>jewelry, watch, c...</td><td>b</td><td>4.4</td></tr>\n",
       "<tr><td>Ultricies Digniss...</td><td>10187291046</td><td>wAtch, clock, and...</td><td>b</td><td>3.29</td></tr>\n",
       "<tr><td>Enim Condimentum PC</td><td>10192359162</td><td>music shops - mus...</td><td>a</td><td>6.33</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+------------+--------------------+-------------+---------+\n",
       "|                name|merchant_abn|            category|revenue_level|take_rate|\n",
       "+--------------------+------------+--------------------+-------------+---------+\n",
       "|       Felis Limited| 10023283211|furniture, home f...|            e|     0.18|\n",
       "|Arcu Ac Orci Corp...| 10142254217|cable, satellite,...|            b|     4.22|\n",
       "|    Nunc Sed Company| 10165489824|jewelry, watch, c...|            b|      4.4|\n",
       "|Ultricies Digniss...| 10187291046|wAtch, clock, and...|            b|     3.29|\n",
       "| Enim Condimentum PC| 10192359162|music shops - mus...|            a|     6.33|\n",
       "+--------------------+------------+--------------------+-------------+---------+"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load merchant's info and clean it\n",
    "merchant_info = spark.read.parquet(f\"{raw_path}/tables/tbl_merchants.parquet\")\n",
    "merchant_info = merchant_info_clean(merchant_info)\n",
    "merchant_info.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>revenue_levels</th><th>count</th></tr>\n",
       "<tr><td>e</td><td>53</td></tr>\n",
       "<tr><td>d</td><td>98</td></tr>\n",
       "<tr><td>c</td><td>922</td></tr>\n",
       "<tr><td>b</td><td>1351</td></tr>\n",
       "<tr><td>a</td><td>1602</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------+-----+\n",
       "|revenue_levels|count|\n",
       "+--------------+-----+\n",
       "|             e|   53|\n",
       "|             d|   98|\n",
       "|             c|  922|\n",
       "|             b| 1351|\n",
       "|             a| 1602|\n",
       "+--------------+-----+"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchant_info.groupBy(\"revenue_levels\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>gender</th><th>count</th></tr>\n",
       "<tr><td>Undisclosed</td><td>50074</td></tr>\n",
       "<tr><td>Female</td><td>224946</td></tr>\n",
       "<tr><td>Male</td><td>224979</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------+------+\n",
       "|     gender| count|\n",
       "+-----------+------+\n",
       "|Undisclosed| 50074|\n",
       "|     Female|224946|\n",
       "|       Male|224979|\n",
       "+-----------+------+"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load consumer's info and reformat\n",
    "consumer_info = spark.read.csv(f\"{raw_path}/tables/tbl_consumer.csv\", header=True, inferSchema=True)\n",
    "consumer_info = consumer_info.withColumn(\"info\", F.split(F.col(\"name|address|state|postcode|gender|consumer_id\"), \"\\|\")).drop(F.col(\"name|address|state|postcode|gender|consumer_id\"))\n",
    "consumer_info = consumer_info.withColumns({\"consumer_id\": F.col(\"info\").getItem(5),\n",
    "                                           \"name\": F.col(\"info\").getItem(0),\n",
    "                                           \"postcode\": F.col(\"info\").getItem(3).cast(IntegerType()),\n",
    "                                           \"gender\": F.col(\"info\").getItem(4)}).drop(F.col(\"info\"))\n",
    "consumer_info.groupBy(\"gender\").count() # relatively same proportion of female and male customer, only a small percentage of did not provide their gender"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
