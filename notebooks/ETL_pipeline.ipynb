{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading library\n",
    "from functools import reduce\n",
    "from pyspark.sql import SparkSession, DataFrame, functions as F\n",
    "from pyspark.sql.types import DoubleType, StringType, IntegerType\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Create a Spark Session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"ETL Pipeline\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.execturo.memory\", \"2g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can't use `urlretrieve` to get the data from Canvas, please download it to your local machine and move it `data/tables`. Then run the code below to unzip the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign data path\n",
    "raw_path = \"../data\"\n",
    "\n",
    "# for file in os.listdir(f\"{raw_path}/tables\"):\n",
    "#     if file == \".gitkeep\":\n",
    "#         continue\n",
    "#     with zipfile.ZipFile(f\"{raw_path}/tables/{file}\", \"r\") as zip_ref:\n",
    "#         zip_ref.extractall(f\"{raw_path}/\")\n",
    "#     os.remove(f\"{raw_path}/tables/{file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform\n",
    "\n",
    "The system use `user_id` as a key for identifying customer in transactions record and fraud probability tables. However, they also have a key-value map of `user_id` and `consumer_id`. We will use `consumer_id` as the only ID for customer. Thus, we will map `user_id` from each table to `consumer_id` and drop the former.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_id(map_df, target_df):\n",
    "    mapped_df = target_df.join(map_df, on=\"user_id\", how=\"inner\")\n",
    "    mapped_df = mapped_df.drop('user_id')\n",
    "    \n",
    "    return mapped_df\n",
    "\n",
    "def merchant_info_clean(df):\n",
    "    df = df.withColumn(\"tags\", F.regexp_replace(\"tags\", r\"^[\\(\\[]|[\\)\\]]$\", \"\")) # Remove the outermost bracket\n",
    "    df = df.withColumn(\"tags\", F.regexp_replace(\"tags\", r\"[\\)\\]],\\s*[\\(\\[]\", r\")\\|(\")) # Replacing the comma that seperate each touple/list into \"|\"\n",
    "\n",
    "# Split accorddingly \n",
    "    df = df.withColumn(\"tags\", F.split(\"tags\", \"\\|\")) \n",
    "    df = df.withColumns({\"category\": F.regexp_replace(F.col(\"tags\").getItem(0), r\"^[\\(\\[]|[\\)\\]]$\", \"\"),\n",
    "                         \"revenue_level\": F.regexp_replace(F.col(\"tags\").getItem(1), r\"^[\\(\\[]|[\\)\\]]$\", \"\"),\n",
    "                         \"take_rate\": F.regexp_extract(F.col(\"tags\").getItem(2), r\"take rate: (\\d+\\.\\d+)\",1).cast(DoubleType())\n",
    "                        })\n",
    "    \n",
    "    df = df.withColumn(\"category\", F.lower(F.col(\"category\")))\n",
    "\n",
    "    df = df.drop(\"tags\")\n",
    "\n",
    "    df = df.filter((F.col(\"revenue_level\") == \"a\") | (F.col(\"revenue_level\") == \"b\") | (F.col(\"revenue_level\") == \"c\") |\n",
    "                   (F.col(\"revenue_level\") == \"d\") | (F.col(\"revenue_level\") == \"e\"))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499999"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load consumer info - a key : value map for user_id to consumer_id\n",
    "consumer_info = spark.read.parquet(f\"{raw_path}/tables/consumer_user_details.parquet\")\n",
    "consumer_info.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all files that need to replace user_id\n",
    "consumer_fraud_rate = spark.read.csv(f\"{raw_path}/tables/consumer_fraud_probability.csv\", header=True, inferSchema=True)\n",
    "consumer_fraud_rate = replace_id(consumer_info, consumer_fraud_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transaction data for user_id replacement\n",
    "transaction_p1 = spark.read.parquet(f\"{raw_path}/tables/transactions_20210228_20210827_snapshot\")\n",
    "transaction_p1 = replace_id(consumer_info, transaction_p1)\n",
    "\n",
    "transaction_p2 = spark.read.parquet(f\"{raw_path}/tables/transactions_20210828_20220227_snapshot\")\n",
    "transaction_p2 = replace_id(consumer_info, transaction_p2)\n",
    "\n",
    "transaction_p3 = spark.read.parquet(f\"{raw_path}/tables/transactions_20220228_20220828_snapshot\")\n",
    "transaction_p3 = replace_id(consumer_info, transaction_p3)\n",
    "\n",
    "transaction_records = reduce(DataFrame.unionAll, [transaction_p1, transaction_p2, transaction_p3])\n",
    "transaction_records.groupBy(\"merchant_abn\").agg(\n",
    "    F.sum(F.col(\"dollar_value\")).alias(\"total_dollar_value\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that replacing `user_id` to `consumer_id` is done, load all other data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load merchant fraud probability\n",
    "merchant_fraud_rate = spark.read.csv(f\"{raw_path}/tables/merchant_fraud_probability.csv\", header=True, inferSchema=True)\n",
    "\n",
    "date_pattern = r\"^\\d{4}-\\d{2}-\\d{2}$\"\n",
    "\n",
    "test = merchant_fraud_rate.withColumn(\"is_valid_date\", F.regexp_extract(F.col(\"order_datetime\"), date_pattern, 0))\n",
    "invalid_dates = test.filter(F.col(\"is_valid_date\") == \"\")\n",
    "invalid_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>merchant_abn</th><th>order_datetime</th><th>fraud_probability</th></tr>\n",
       "<tr><td>19492220327</td><td>2021-11-28</td><td>44.403658647495355</td></tr>\n",
       "<tr><td>31334588839</td><td>2021-10-02</td><td>42.75530083865367</td></tr>\n",
       "<tr><td>19492220327</td><td>2021-12-22</td><td>38.867790051131095</td></tr>\n",
       "<tr><td>82999039227</td><td>2021-12-19</td><td>94.1347004808891</td></tr>\n",
       "<tr><td>90918180829</td><td>2021-09-02</td><td>43.32551731714902</td></tr>\n",
       "<tr><td>31334588839</td><td>2021-12-26</td><td>38.36165958070444</td></tr>\n",
       "<tr><td>23686790459</td><td>2021-12-10</td><td>79.4543441508535</td></tr>\n",
       "<tr><td>14827550074</td><td>2021-11-26</td><td>46.45775596795885</td></tr>\n",
       "<tr><td>31334588839</td><td>2021-11-26</td><td>36.20971272078342</td></tr>\n",
       "<tr><td>19492220327</td><td>2021-12-18</td><td>33.819672154331755</td></tr>\n",
       "<tr><td>31334588839</td><td>2021-11-29</td><td>35.386213297375505</td></tr>\n",
       "<tr><td>14827550074</td><td>2021-12-05</td><td>43.85519494291279</td></tr>\n",
       "<tr><td>19492220327</td><td>2021-11-18</td><td>32.193139919494016</td></tr>\n",
       "<tr><td>93260930990</td><td>2021-11-30</td><td>37.87197154172081</td></tr>\n",
       "<tr><td>90918180829</td><td>2021-09-16</td><td>36.62001350882694</td></tr>\n",
       "<tr><td>83199298021</td><td>2022-02-27</td><td>26.025158824861773</td></tr>\n",
       "<tr><td>83199298021</td><td>2022-02-17</td><td>25.77998392496447</td></tr>\n",
       "<tr><td>94311056026</td><td>2021-12-16</td><td>30.85150199484772</td></tr>\n",
       "<tr><td>97217894162</td><td>2022-01-21</td><td>34.94582650821017</td></tr>\n",
       "<tr><td>43083074133</td><td>2021-12-18</td><td>48.66890735193894</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+------------+--------------+------------------+\n",
       "|merchant_abn|order_datetime| fraud_probability|\n",
       "+------------+--------------+------------------+\n",
       "| 19492220327|    2021-11-28|44.403658647495355|\n",
       "| 31334588839|    2021-10-02| 42.75530083865367|\n",
       "| 19492220327|    2021-12-22|38.867790051131095|\n",
       "| 82999039227|    2021-12-19|  94.1347004808891|\n",
       "| 90918180829|    2021-09-02| 43.32551731714902|\n",
       "| 31334588839|    2021-12-26| 38.36165958070444|\n",
       "| 23686790459|    2021-12-10|  79.4543441508535|\n",
       "| 14827550074|    2021-11-26| 46.45775596795885|\n",
       "| 31334588839|    2021-11-26| 36.20971272078342|\n",
       "| 19492220327|    2021-12-18|33.819672154331755|\n",
       "| 31334588839|    2021-11-29|35.386213297375505|\n",
       "| 14827550074|    2021-12-05| 43.85519494291279|\n",
       "| 19492220327|    2021-11-18|32.193139919494016|\n",
       "| 93260930990|    2021-11-30| 37.87197154172081|\n",
       "| 90918180829|    2021-09-16| 36.62001350882694|\n",
       "| 83199298021|    2022-02-27|26.025158824861773|\n",
       "| 83199298021|    2022-02-17| 25.77998392496447|\n",
       "| 94311056026|    2021-12-16| 30.85150199484772|\n",
       "| 97217894162|    2022-01-21| 34.94582650821017|\n",
       "| 43083074133|    2021-12-18| 48.66890735193894|\n",
       "+------------+--------------+------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchant_fraud_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning `tbl_merchants.parquet`. The feature `tags` is a string that represent either a tuple or a list, containing 3 elements:\n",
    "* Items that are being sold\n",
    "* Revenue levels\n",
    "* Commission rate\n",
    "Each elements either a list, a tuple, or a combination of both (e.g starts with `[` and ends with `)` and vice versa). These inconsistencies are mostly due to human errors. Thus, we need to take into account these consistent when splitting the values of the feature `tags` into separate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>revenue_level</th><th>avg(take_rate)</th></tr>\n",
       "<tr><td>e</td><td>0.3147169811320755</td></tr>\n",
       "<tr><td>d</td><td>0.9912244897959185</td></tr>\n",
       "<tr><td>c</td><td>2.2512039045553167</td></tr>\n",
       "<tr><td>b</td><td>4.094056254626199</td></tr>\n",
       "<tr><td>a</td><td>6.232297128589269</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------------+------------------+\n",
       "|revenue_level|    avg(take_rate)|\n",
       "+-------------+------------------+\n",
       "|            e|0.3147169811320755|\n",
       "|            d|0.9912244897959185|\n",
       "|            c|2.2512039045553167|\n",
       "|            b| 4.094056254626199|\n",
       "|            a| 6.232297128589269|\n",
       "+-------------+------------------+"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load merchant's info and clean it\n",
    "merchant_info = spark.read.parquet(f\"{raw_path}/tables/tbl_merchants.parquet\")\n",
    "merchant_info = merchant_info_clean(merchant_info)\n",
    "merchant_info.groupBy(F.col(\"revenue_level\")).agg(F.avg(F.col(\"take_rate\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4026"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merchant_info.select(\"merchant_abn\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>gender</th><th>count</th></tr>\n",
       "<tr><td>Undisclosed</td><td>50074</td></tr>\n",
       "<tr><td>Female</td><td>224946</td></tr>\n",
       "<tr><td>Male</td><td>224979</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------+------+\n",
       "|     gender| count|\n",
       "+-----------+------+\n",
       "|Undisclosed| 50074|\n",
       "|     Female|224946|\n",
       "|       Male|224979|\n",
       "+-----------+------+"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load consumer's info and reformat\n",
    "consumer_info = spark.read.csv(f\"{raw_path}/tables/tbl_consumer.csv\", header=True, inferSchema=True)\n",
    "consumer_info = consumer_info.withColumn(\"info\", F.split(F.col(\"name|address|state|postcode|gender|consumer_id\"), \"\\|\")).drop(F.col(\"name|address|state|postcode|gender|consumer_id\"))\n",
    "consumer_info = consumer_info.withColumns({\"consumer_id\": F.col(\"info\").getItem(5),\n",
    "                                           \"name\": F.col(\"info\").getItem(0),\n",
    "                                           \"postcode\": F.col(\"info\").getItem(3).cast(IntegerType()),\n",
    "                                           \"gender\": F.col(\"info\").getItem(4)}).drop(F.col(\"info\"))\n",
    "consumer_info.groupBy(\"gender\").count() # relatively same proportion of female and male customer, only a small percentage of did not provide their gender"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
