{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading library\n",
    "import os\n",
    "os.sys.path.append(\"../\")\n",
    "from scripts.etl_pipeline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 18:01:30 WARN Utils: Your hostname, DESKTOP-H6V94HM resolves to a loopback address: 127.0.1.1; using 192.168.0.102 instead (on interface eth0)\n",
      "24/08/29 18:01:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/29 18:01:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark Session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"ETL Pipeline\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.execturo.memory\", \"2g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can't use `urlretrieve` to get the data from Canvas, please download it to your local machine and move it `data/tables`. Then run the code below to unzip the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign data path\n",
    "raw_path = \"../data\"\n",
    "\n",
    "for file in os.listdir(f\"{raw_path}/tables\"):\n",
    "    if file == \".gitkeep\":\n",
    "        continue\n",
    "    with zipfile.ZipFile(f\"{raw_path}/tables/{file}\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(f\"{raw_path}/\")\n",
    "    os.remove(f\"{raw_path}/tables/{file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform\n",
    "\n",
    "The system use `user_id` as a key for identifying customer in transactions record and fraud probability tables. However, they also have a key-value map of `user_id` and `consumer_id`. We will use `consumer_id` as the only ID for customer. Thus, we will map `user_id` from each table to `consumer_id` and drop the former.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load consumer info - a key : value map for user_id to consumer_id\n",
    "consumer_info = spark.read.parquet(f\"{raw_path}/tables/consumer_user_details.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all files that need to replace user_id\n",
    "consumer_fraud_rate = spark.read.csv(f\"{raw_path}/tables/consumer_fraud_probability.csv\", header=True, inferSchema=True)\n",
    "consumer_fraud_rate = replace_id(consumer_info, consumer_fraud_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transaction data for user_id replacement\n",
    "transaction_p1 = spark.read.parquet(f\"{raw_path}/tables/transactions_20210228_20210827_snapshot\")\n",
    "transaction_p1 = replace_id(consumer_info, transaction_p1)\n",
    "\n",
    "transaction_p2 = spark.read.parquet(f\"{raw_path}/tables/transactions_20210828_20220227_snapshot\")\n",
    "transaction_p2 = replace_id(consumer_info, transaction_p2)\n",
    "\n",
    "transaction_p3 = spark.read.parquet(f\"{raw_path}/tables/transactions_20220228_20220828_snapshot\")\n",
    "transaction_p3 = replace_id(consumer_info, transaction_p3)\n",
    "\n",
    "transaction_records = reduce(DataFrame.unionAll, [transaction_p1, transaction_p2, transaction_p3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that replacing `user_id` to `consumer_id` is done, load all other data and clean them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load merchant fraud probability\n",
    "# merchant_fraud_rate = spark.read.csv(f\"{raw_path}/tables/merchant_fraud_probability.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# date_pattern = r\"^\\d{4}-\\d{2}-\\d{2}$\"\n",
    "\n",
    "# test = merchant_fraud_rate.withColumn(\"is_valid_date\", F.regexp_extract(F.col(\"order_datetime\"), date_pattern, 0))\n",
    "# invalid_dates = test.filter(F.col(\"is_valid_date\") == \"\")\n",
    "# invalid_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of merchants: 4422\n",
      "Total number of merchants with fraudulent probability: 61\n"
     ]
    }
   ],
   "source": [
    "merchant_fraud = spark.read.csv(f\"{raw_path}/tables/merchant_fraud_probability.csv\", header=True, inferSchema=True)\n",
    "print(f'Total number of merchants: {transaction_records.select(\"merchant_abn\").distinct().count()}')\n",
    "print(f'Total number of merchants with fraudulent probability: {merchant_fraud.select(\"merchant_abn\").distinct().count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning `tbl_merchants.parquet`. The feature `tags` is a string that represent either a tuple or a list, containing 3 elements:\n",
    "* Items that are being sold\n",
    "* Revenue levels\n",
    "* Commission rate\n",
    "Each elements either a list, a tuple, or a combination of both (e.g starts with `[` and ends with `)` and vice versa). These inconsistencies are mostly due to human errors. Thus, we need to take into account these consistent when splitting the values of the feature `tags` into separate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- merchant_abn: long (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- revenue_level: string (nullable = true)\n",
      " |-- take_rate: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>name</th><th>merchant_abn</th><th>category</th><th>revenue_level</th><th>take_rate</th></tr>\n",
       "<tr><td>Felis Limited</td><td>10023283211</td><td>furniture, home f...</td><td>e</td><td>0.18</td></tr>\n",
       "<tr><td>Arcu Ac Orci Corp...</td><td>10142254217</td><td>cable, satellite,...</td><td>b</td><td>4.22</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+------------+--------------------+-------------+---------+\n",
       "|                name|merchant_abn|            category|revenue_level|take_rate|\n",
       "+--------------------+------------+--------------------+-------------+---------+\n",
       "|       Felis Limited| 10023283211|furniture, home f...|            e|     0.18|\n",
       "|Arcu Ac Orci Corp...| 10142254217|cable, satellite,...|            b|     4.22|\n",
       "+--------------------+------------+--------------------+-------------+---------+"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load merchant's info and clean it\n",
    "merchant_info = spark.read.parquet(f\"{raw_path}/tables/tbl_merchants.parquet\")\n",
    "merchant_info = load_merchant_details(merchant_info)\n",
    "merchant_info.printSchema()\n",
    "merchant_info.limit(2)\n",
    "# merchant_info.groupBy(F.col(\"revenue_level\")).agg(F.avg(F.col(\"take_rate\"))) # average commission rate of each revenue level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>name</th><th>consumer_id</th><th>gender</th><th>state</th><th>postcode</th></tr>\n",
       "<tr><td>Yolanda Williams</td><td>1195503</td><td>Female</td><td>WA</td><td>6935</td></tr>\n",
       "<tr><td>Mary Smith</td><td>179208</td><td>Female</td><td>NSW</td><td>2782</td></tr>\n",
       "<tr><td>Jill Jones MD</td><td>1194530</td><td>Female</td><td>NT</td><td>862</td></tr>\n",
       "<tr><td>Lindsay Jimenez</td><td>154128</td><td>Female</td><td>NSW</td><td>2780</td></tr>\n",
       "<tr><td>Rebecca Blanchard</td><td>712975</td><td>Female</td><td>WA</td><td>6355</td></tr>\n",
       "<tr><td>Karen Chapman</td><td>407340</td><td>Female</td><td>NSW</td><td>2033</td></tr>\n",
       "<tr><td>Andrea Jones</td><td>511685</td><td>Female</td><td>QLD</td><td>4606</td></tr>\n",
       "<tr><td>Stephen Williams</td><td>448088</td><td>Male</td><td>WA</td><td>6056</td></tr>\n",
       "<tr><td>Stephanie Reyes</td><td>650435</td><td>Female</td><td>NSW</td><td>2482</td></tr>\n",
       "<tr><td>Jillian Gonzales</td><td>1058499</td><td>Female</td><td>VIC</td><td>3220</td></tr>\n",
       "<tr><td>Eugene Lucas</td><td>428325</td><td>Undisclosed</td><td>VIC</td><td>3063</td></tr>\n",
       "<tr><td>Melissa Jones</td><td>1494640</td><td>Female</td><td>WA</td><td>6743</td></tr>\n",
       "<tr><td>Angela Brown PhD</td><td>1146717</td><td>Female</td><td>QLD</td><td>4673</td></tr>\n",
       "<tr><td>Lance Butler</td><td>1343547</td><td>Male</td><td>VIC</td><td>3332</td></tr>\n",
       "<tr><td>Paul Abbott</td><td>1463076</td><td>Male</td><td>QLD</td><td>4512</td></tr>\n",
       "<tr><td>Tracy Hart</td><td>1356405</td><td>Male</td><td>NSW</td><td>2452</td></tr>\n",
       "<tr><td>Alyssa Wilson</td><td>1331093</td><td>Female</td><td>VIC</td><td>3719</td></tr>\n",
       "<tr><td>Michael Burnett</td><td>80965</td><td>Male</td><td>NSW</td><td>1109</td></tr>\n",
       "<tr><td>Victoria Gonzalez</td><td>1226530</td><td>Female</td><td>TAS</td><td>7276</td></tr>\n",
       "<tr><td>James Norris</td><td>1390367</td><td>Undisclosed</td><td>VIC</td><td>3234</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+-----------------+-----------+-----------+-----+--------+\n",
       "|             name|consumer_id|     gender|state|postcode|\n",
       "+-----------------+-----------+-----------+-----+--------+\n",
       "| Yolanda Williams|    1195503|     Female|   WA|    6935|\n",
       "|       Mary Smith|     179208|     Female|  NSW|    2782|\n",
       "|    Jill Jones MD|    1194530|     Female|   NT|     862|\n",
       "|  Lindsay Jimenez|     154128|     Female|  NSW|    2780|\n",
       "|Rebecca Blanchard|     712975|     Female|   WA|    6355|\n",
       "|    Karen Chapman|     407340|     Female|  NSW|    2033|\n",
       "|     Andrea Jones|     511685|     Female|  QLD|    4606|\n",
       "| Stephen Williams|     448088|       Male|   WA|    6056|\n",
       "|  Stephanie Reyes|     650435|     Female|  NSW|    2482|\n",
       "| Jillian Gonzales|    1058499|     Female|  VIC|    3220|\n",
       "|     Eugene Lucas|     428325|Undisclosed|  VIC|    3063|\n",
       "|    Melissa Jones|    1494640|     Female|   WA|    6743|\n",
       "| Angela Brown PhD|    1146717|     Female|  QLD|    4673|\n",
       "|     Lance Butler|    1343547|       Male|  VIC|    3332|\n",
       "|      Paul Abbott|    1463076|       Male|  QLD|    4512|\n",
       "|       Tracy Hart|    1356405|       Male|  NSW|    2452|\n",
       "|    Alyssa Wilson|    1331093|     Female|  VIC|    3719|\n",
       "|  Michael Burnett|      80965|       Male|  NSW|    1109|\n",
       "|Victoria Gonzalez|    1226530|     Female|  TAS|    7276|\n",
       "|     James Norris|    1390367|Undisclosed|  VIC|    3234|\n",
       "+-----------------+-----------+-----------+-----+--------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load consumer's info and reformat\n",
    "consumer_info = spark.read.csv(f\"{raw_path}/tables/tbl_consumer.csv\", header=True, inferSchema=True)\n",
    "consumer_info = load_consumer_details(consumer_info)\n",
    "\n",
    "# consumer_info.groupBy(\"gender\").count() # relatively same proportion of female and male customer, only a small percentage of did not provide their gender\n",
    "consumer_info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
