{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading library\n",
        "import os\n",
        "os.sys.path.append(\"../\")\n",
        "from scripts.etl_pipeline import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/09/10 18:43:35 WARN Utils: Your hostname, Alistairs-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 10.13.156.189 instead (on interface en0)\n",
            "24/09/10 18:43:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "24/09/10 18:43:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Create a Spark Session\n",
        "spark = (\n",
        "    SparkSession.builder.appName(\"ETL Pipeline\")\n",
        "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
        "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
        "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
        "    .config(\"spark.driver.memory\", \"4g\")\n",
        "    .config(\"spark.executor.memory\", \"2g\")\n",
        "    .getOrCreate()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Extract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can't use `urlretrieve` to get the data from Canvas, please download it to your local machine and move it `data/tables`. Then run the code below to unzip the files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nfor file in os.listdir(f\"{raw_path}/tables\"):\\n    if file == \".gitkeep\":\\n        continue\\n    with zipfile.ZipFile(f\"{raw_path}/tables/{file}\", \"r\") as zip_ref:\\n        zip_ref.extractall(f\"{raw_path}/\")\\n    os.remove(f\"{raw_path}/tables/{file}\")\\n'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Assign data path\n",
        "raw_path = \"../data\"\n",
        "\n",
        "# Unzip files (Only run once)\n",
        "\"\"\"\n",
        "for file in os.listdir(f\"{raw_path}/tables\"):\n",
        "    if file == \".gitkeep\":\n",
        "        continue\n",
        "    with zipfile.ZipFile(f\"{raw_path}/tables/{file}\", \"r\") as zip_ref:\n",
        "        zip_ref.extractall(f\"{raw_path}/\")\n",
        "    os.remove(f\"{raw_path}/tables/{file}\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transform\n",
        "\n",
        "The system use `user_id` as a key for identifying customer in transactions record and fraud probability tables. However, they also have a key-value map of `user_id` and `consumer_id`. We will use `consumer_id` as the only ID for customer. Thus, we will map `user_id` from each table to `consumer_id` and drop the former.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border='1'>\n",
              "<tr><th>user_id</th><th>consumer_id</th></tr>\n",
              "<tr><td>1</td><td>1195503</td></tr>\n",
              "<tr><td>2</td><td>179208</td></tr>\n",
              "<tr><td>3</td><td>1194530</td></tr>\n",
              "<tr><td>4</td><td>154128</td></tr>\n",
              "<tr><td>5</td><td>712975</td></tr>\n",
              "</table>\n"
            ],
            "text/plain": [
              "+-------+-----------+\n",
              "|user_id|consumer_id|\n",
              "+-------+-----------+\n",
              "|      1|    1195503|\n",
              "|      2|     179208|\n",
              "|      3|    1194530|\n",
              "|      4|     154128|\n",
              "|      5|     712975|\n",
              "+-------+-----------+"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load consumer user details -> a key:value map for user_id to consumer_id\n",
        "consumer_user_map = spark.read.parquet(f\"{raw_path}/tables/consumer_user_details.parquet\")\n",
        "consumer_user_map.limit(5) # Preview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+--------------+-----------------+\n",
            "|user_id|order_datetime|fraud_probability|\n",
            "+-------+--------------+-----------------+\n",
            "|   6228|    2021-12-19| 97.6298077657765|\n",
            "|  21419|    2021-12-10|99.24738020302328|\n",
            "|   5606|    2021-10-17|84.05825045251777|\n",
            "|   3101|    2021-04-17|91.42192091901347|\n",
            "|  22239|    2021-10-19|94.70342477508035|\n",
            "+-------+--------------+-----------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "The dataset count is  34864\n",
            "+--------------+------------------+-----------+\n",
            "|order_datetime| fraud_probability|consumer_id|\n",
            "+--------------+------------------+-----------+\n",
            "|    2022-02-20| 9.805431136520959|    1195503|\n",
            "|    2021-08-30| 9.599513915425788|     179208|\n",
            "|    2021-09-25|10.069850934775245|     179208|\n",
            "|    2021-11-03| 8.300636455314633|    1194530|\n",
            "|    2021-10-09| 9.633302411090419|     154128|\n",
            "+--------------+------------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "The dataset count is  34864\n"
          ]
        }
      ],
      "source": [
        "# Load consumer fraud rate dataset\n",
        "consumer_fraud_rate = spark.read.csv(f\"{raw_path}/tables/consumer_fraud_probability.csv\", header=True, inferSchema=True)\n",
        "consumer_fraud_rate.show(5)\n",
        "get_dataset_count(consumer_fraud_rate)\n",
        "\n",
        "# Replace all user_id with unique consumer_id\n",
        "consumer_fraud_rate = replace_id(consumer_user_map, consumer_fraud_rate)\n",
        "consumer_fraud_rate.show(5)\n",
        "\n",
        "# Check to make sure no rows were lost on the inner join\n",
        "get_dataset_count(consumer_fraud_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The dataset count is  14195505\n",
            "+------------+------------------+--------------------+--------------+-----------+\n",
            "|merchant_abn|      dollar_value|            order_id|order_datetime|consumer_id|\n",
            "+------------+------------------+--------------------+--------------+-----------+\n",
            "| 62191208634|63.255848959735246|949a63c8-29f7-4ab...|    2021-08-20|     651338|\n",
            "| 15549624934| 130.3505283105634|6a84c3cf-612a-457...|    2021-08-20|     179208|\n",
            "| 64403598239|120.15860593212783|b10dcc33-e53f-425...|    2021-08-20|     467663|\n",
            "| 60956456424| 136.6785200286976|0f09c5a5-784e-447...|    2021-08-20|    1194530|\n",
            "| 94493496784| 72.96316578355305|f6c78c1a-4600-4c5...|    2021-08-20|     467663|\n",
            "+------------+------------------+--------------------+--------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 42:==================================>                     (16 + 8) / 26]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The dataset count is  14195505\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Load all the transaction data \n",
        "transaction_p1 = spark.read.parquet(f\"{raw_path}/tables/transactions_20210228_20210827_snapshot\")\n",
        "transaction_p2 = spark.read.parquet(f\"{raw_path}/tables/transactions_20210828_20220227_snapshot\")\n",
        "transaction_p3 = spark.read.parquet(f\"{raw_path}/tables/transactions_20220228_20220828_snapshot\")\n",
        "\n",
        "# Combine the datasets\n",
        "transaction_records = reduce(DataFrame.unionAll, [transaction_p1, transaction_p2, transaction_p3])\n",
        "# transaction_records.show(5)\n",
        "get_dataset_count(transaction_records)\n",
        "\n",
        "# Replace user_id with consumer_id after combining\n",
        "transaction_records = replace_id(consumer_user_map, transaction_records)\n",
        "transaction_records.show(5)\n",
        "\n",
        "# Check to make sure no rows were lost on the inner join\n",
        "get_dataset_count(transaction_records)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that replacing `user_id` to `consumer_id` is done, load all other data and clean them. First, let's have a look at the merchant data on fraud probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+--------------+------------------+\n",
            "|merchant_abn|order_datetime| fraud_probability|\n",
            "+------------+--------------+------------------+\n",
            "| 19492220327|    2021-11-28|44.403658647495355|\n",
            "| 31334588839|    2021-10-02| 42.75530083865367|\n",
            "| 19492220327|    2021-12-22|38.867790051131095|\n",
            "| 82999039227|    2021-12-19|  94.1347004808891|\n",
            "| 90918180829|    2021-09-02| 43.32551731714902|\n",
            "+------------+--------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "The dataset count is  114\n"
          ]
        }
      ],
      "source": [
        "# Load merchant fraud rate dataset\n",
        "merchant_fraud_rate = spark.read.csv(f\"{raw_path}/tables/merchant_fraud_probability.csv\", header=True, inferSchema=True)\n",
        "merchant_fraud_rate.show(5)\n",
        "get_dataset_count(merchant_fraud_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cleaning `tbl_merchants.parquet`. The feature `tags` is a string that represents either a tuple or a list, containing 3 elements:\n",
        "* Items that are being sold\n",
        "* Revenue levels\n",
        "* Commission rate\n",
        "\n",
        "Each elements either a list, a tuple, or a combination of both (e.g starts with `[` and ends with `)` and vice versa). These inconsistencies are mostly due to human errors. Thus, we need to take into account these consistent when splitting the values of the feature `tags` into separate columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------------------+-----------------------------------------------------------------------------------------------------------------+------------+\n",
            "|name                                |tags                                                                                                             |merchant_abn|\n",
            "+------------------------------------+-----------------------------------------------------------------------------------------------------------------+------------+\n",
            "|Felis Limited                       |((furniture, home furnishings and equipment shops, and manufacturers, except appliances), (e), (take rate: 0.18))|10023283211 |\n",
            "|Arcu Ac Orci Corporation            |([cable, satellite, and otHer pay television and radio services], [b], [take rate: 4.22])                        |10142254217 |\n",
            "|Nunc Sed Company                    |([jewelry, watch, clock, and silverware shops], [b], [take rate: 4.40])                                          |10165489824 |\n",
            "|Ultricies Dignissim Lacus Foundation|([wAtch, clock, and jewelry repair shops], [b], [take rate: 3.29])                                               |10187291046 |\n",
            "|Enim Condimentum PC                 |([music shops - musical instruments, pianos, and sheet music], [a], [take rate: 6.33])                           |10192359162 |\n",
            "+------------------------------------+-----------------------------------------------------------------------------------------------------------------+------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Before: \n",
            "The dataset count is  4026\n",
            "After: \n",
            "The dataset count is  4026\n",
            "+------------------------------------+------------+-------------------------------------------------------------------------------------+-------------+---------+\n",
            "|name                                |merchant_abn|category                                                                             |revenue_level|take_rate|\n",
            "+------------------------------------+------------+-------------------------------------------------------------------------------------+-------------+---------+\n",
            "|Felis Limited                       |10023283211 |furniture, home furnishings and equipment shops, and manufacturers, except appliances|e            |0.18     |\n",
            "|Arcu Ac Orci Corporation            |10142254217 |cable, satellite, and other pay television and radio services                        |b            |4.22     |\n",
            "|Nunc Sed Company                    |10165489824 |jewelry, watch, clock, and silverware shops                                          |b            |4.4      |\n",
            "|Ultricies Dignissim Lacus Foundation|10187291046 |watch, clock, and jewelry repair shops                                               |b            |3.29     |\n",
            "|Enim Condimentum PC                 |10192359162 |music shops - musical instruments, pianos, and sheet music                           |a            |6.33     |\n",
            "+------------------------------------+------------+-------------------------------------------------------------------------------------+-------------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load merchant's info\n",
        "merchant_info = spark.read.parquet(f\"{raw_path}/tables/tbl_merchants.parquet\")\n",
        "merchant_info.show(5, truncate=False)\n",
        "\n",
        "# Clean the data\n",
        "merchant_info = clean_merchant_details(merchant_info)\n",
        "merchant_info.show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data on consumer's basic information is a single column that contains the consumer's name, address, state, postcode, gender, and their unqiue consumer ID, each separated by \"`|`\". Thus, we will need to split these into individual columns. Based on the `README.md` for the data, we will only keep the consumer's name, state, postcode, gender, and consumer ID as the addresses are fake."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------------------------------------------------------------+\n",
            "|name|address|state|postcode|gender|consumer_id                       |\n",
            "+---------------------------------------------------------------------+\n",
            "|Yolanda Williams|413 Haney Gardens Apt. 742|WA|6935|Female|1195503   |\n",
            "|Mary Smith|3764 Amber Oval|NSW|2782|Female|179208                    |\n",
            "|Jill Jones MD|40693 Henry Greens|NT|862|Female|1194530               |\n",
            "|Lindsay Jimenez|00653 Davenport Crossroad|NSW|2780|Female|154128     |\n",
            "|Rebecca Blanchard|9271 Michael Manors Suite 651|WA|6355|Female|712975|\n",
            "+---------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Before: \n",
            "The dataset count is  499999\n",
            "After: \n",
            "The dataset count is  499999\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<table border='1'>\n",
              "<tr><th>name</th><th>consumer_id</th><th>gender</th><th>state</th><th>postcode</th></tr>\n",
              "<tr><td>Yolanda Williams</td><td>1195503</td><td>Female</td><td>WA</td><td>6935</td></tr>\n",
              "<tr><td>Mary Smith</td><td>179208</td><td>Female</td><td>NSW</td><td>2782</td></tr>\n",
              "<tr><td>Jill Jones MD</td><td>1194530</td><td>Female</td><td>NT</td><td>862</td></tr>\n",
              "<tr><td>Lindsay Jimenez</td><td>154128</td><td>Female</td><td>NSW</td><td>2780</td></tr>\n",
              "<tr><td>Rebecca Blanchard</td><td>712975</td><td>Female</td><td>WA</td><td>6355</td></tr>\n",
              "</table>\n"
            ],
            "text/plain": [
              "+-----------------+-----------+------+-----+--------+\n",
              "|             name|consumer_id|gender|state|postcode|\n",
              "+-----------------+-----------+------+-----+--------+\n",
              "| Yolanda Williams|    1195503|Female|   WA|    6935|\n",
              "|       Mary Smith|     179208|Female|  NSW|    2782|\n",
              "|    Jill Jones MD|    1194530|Female|   NT|     862|\n",
              "|  Lindsay Jimenez|     154128|Female|  NSW|    2780|\n",
              "|Rebecca Blanchard|     712975|Female|   WA|    6355|\n",
              "+-----------------+-----------+------+-----+--------+"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load consumer info's\n",
        "consumer_info = spark.read.csv(f\"{raw_path}/tables/tbl_consumer.csv\", header=True, inferSchema=True)\n",
        "consumer_info.show(5, truncate=False)\n",
        "\n",
        "# Clean the data\n",
        "consumer_info = clean_consumer_details(consumer_info)\n",
        "consumer_info.limit(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will also need to ensure that the datetime of all dataset with such column is within the specified range (labeled on the name of the intial downloaded file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting entries: 12561377 \n",
            "Final entries: 12561377\n",
            "Net change (%): 0.0 \n",
            "Starting entries: 114 \n",
            "Final entries: 114\n",
            "Net change (%): 0.0 \n",
            "Starting entries: 34864 \n",
            "Final entries: 34864\n",
            "Net change (%): 0.0 \n"
          ]
        }
      ],
      "source": [
        "transaction_records = ensure_datetime_range(transaction_records, \"2021-02-28\", \"2022-08-28\")\n",
        "\n",
        "merchant_fraud_rate = ensure_datetime_range(merchant_fraud_rate, \"2021-02-08\", \"2022-08-28\")\n",
        "\n",
        "consumer_fraud_rate = ensure_datetime_range(consumer_fraud_rate, \"2021-02-08\", \"2022-08-28\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next,  we check for any existing null values across all dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------+-------------------------------+-------------------------+\n",
            "|order_datetime_missing_count|fraud_probability_missing_count|consumer_id_missing_count|\n",
            "+----------------------------+-------------------------------+-------------------------+\n",
            "|                           0|                              0|                        0|\n",
            "+----------------------------+-------------------------------+-------------------------+\n",
            "\n",
            "+--------------------------+----------------------------+-------------------------------+\n",
            "|merchant_abn_missing_count|order_datetime_missing_count|fraud_probability_missing_count|\n",
            "+--------------------------+----------------------------+-------------------------------+\n",
            "|                         0|                           0|                              0|\n",
            "+--------------------------+----------------------------+-------------------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+-------------------------+--------------------+-------------------+----------------------+\n",
            "|name_missing_count|consumer_id_missing_count|gender_missing_count|state_missing_count|postcode_missing_count|\n",
            "+------------------+-------------------------+--------------------+-------------------+----------------------+\n",
            "|                 0|                        0|                   0|                  0|                     0|\n",
            "+------------------+-------------------------+--------------------+-------------------+----------------------+\n",
            "\n",
            "+------------------+--------------------------+----------------------+---------------------------+-----------------------+\n",
            "|name_missing_count|merchant_abn_missing_count|category_missing_count|revenue_level_missing_count|take_rate_missing_count|\n",
            "+------------------+--------------------------+----------------------+---------------------------+-----------------------+\n",
            "|                 0|                         0|                     0|                          0|                      0|\n",
            "+------------------+--------------------------+----------------------+---------------------------+-----------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 92:=====================================>                  (16 + 8) / 24]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------+--------------------------+----------------------+----------------------------+-------------------------+\n",
            "|merchant_abn_missing_count|dollar_value_missing_count|order_id_missing_count|order_datetime_missing_count|consumer_id_missing_count|\n",
            "+--------------------------+--------------------------+----------------------+----------------------------+-------------------------+\n",
            "|                         0|                         0|                     0|                           0|                        0|\n",
            "+--------------------------+--------------------------+----------------------+----------------------------+-------------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Check for Missing values\n",
        "calculate_missing_values(consumer_fraud_rate)\n",
        "calculate_missing_values(merchant_fraud_rate)\n",
        "calculate_missing_values(consumer_info)\n",
        "calculate_missing_values(merchant_info)\n",
        "calculate_missing_values(transaction_records)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Save\n",
        "\n",
        "We now export the curated into the `data/curated` directory for analysis and modelling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "landing_directory = \"../data/curated\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write cleaned data to disk. (Overwrite will replace any existing files)\n",
        "consumer_fraud_rate.write.mode(\"overwrite\").parquet(f\"{landing_directory}/consumer_fraud_prob.parquet\")\n",
        "merchant_fraud_rate.write.mode(\"overwrite\").parquet(f\"{landing_directory}/merchant_fraud_prob.parquet\")\n",
        "transaction_records.write.mode(\"overwrite\").parquet(f\"{landing_directory}/transactions.parquet\")\n",
        "merchant_info.write.mode(\"overwrite\").parquet(f\"{landing_directory}/merchant_info.parquet\")\n",
        "consumer_info.write.mode(\"overwrite\").parquet(f\"{landing_directory}/consumer_info.parquet\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section, we begin exploratory data analysis to gain a better understanding of the data and our variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Revenue analysis\n",
        "# Explore for each ABN, their total revenue, average order value and total number of orders\n",
        "merchant_performance = transaction_records.groupBy(\"merchant_abn\").agg(\n",
        "    F.sum(\"dollar_value\").alias(\"total_revenue\"),\n",
        "    F.avg(\"dollar_value\").alias(\"average_order_value\"),\n",
        "    F.count(\"dollar_value\").alias(\"total_orders\")\n",
        ")\n",
        "\n",
        "# Rank merchants by total revenue in descending order\n",
        "merchant_performance = merchant_performance.orderBy(F.col(\"total_revenue\").desc())\n",
        "merchant_performance.show(10)\n",
        "\n",
        "get_dataset_count(merchant_performance)\n",
        "calculate_missing_values(merchant_performance)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "merchant_avg_fraud_prob = merchant_fraud_rate.groupBy(\"merchant_abn\").agg(avg(\"fraud_probability\").alias(\"merchant_avg_fraud_prob\"))\n",
        "\n",
        "merchant_avg_fraud_prob.show()\n",
        "\n",
        "calculate_missing_values(merchant_avg_fraud_prob)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Risk Analysis\n",
        "# Combine revenue analysis with fraud probabilities\n",
        "merchant_performance_risk = merchant_performance.join(merchant_avg_fraud_prob, on=\"merchant_abn\", how=\"left\")\n",
        "\n",
        "# Rank merchants by fraud probability in descending order\n",
        "merchant_performance_risk = merchant_performance_risk.orderBy(F.col(\"merchant_avg_fraud_prob\").desc())\n",
        "merchant_performance_risk.show(10)\n",
        "\n",
        "get_dataset_count(merchant_performance_risk)\n",
        "calculate_missing_values(merchant_performance_risk)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Profitability analysis\n",
        "# Combine previous merchant_performance_risk with merchants_info to get take_rate and revenue_level\n",
        "merchant_profitability = merchant_performance_risk.join(merchant_info, on=\"merchant_abn\", how=\"inner\")\n",
        "\n",
        "# Calculate net revenue\n",
        "merchant_profitability = merchant_profitability.withColumn(\n",
        "    \"net_revenue\", F.col(\"total_revenue\") * F.col(\"take_rate\")/100\n",
        ")\n",
        "\n",
        "# Rank merchants by profitability in descending order\n",
        "merchant_profitability.orderBy(F.col(\"net_revenue\").desc()).show(10)\n",
        "\n",
        "\n",
        "# Rank merchants by profitability in ascending order\n",
        "merchant_profitability.orderBy(F.col(\"net_revenue\").asc()).show(10)\n",
        "\n",
        "get_dataset_count(merchant_profitability)\n",
        "calculate_missing_values(merchant_profitability)\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
